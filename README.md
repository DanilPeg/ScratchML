# ScratchML

**ScratchML** — это библиотека классических алгоритмов машинного обучения, реализованных с нуля на numpy.  
Задача - продемонстрировать логику, как по "кирпичикам" собирать базовые модели машинного обучения.

**Цель проекта:**  
Показать, как устроены базовые алгоритмы ML "изнутри", чтобы любой желающий мог разобраться в деталях, поэкспериментировать и научиться строить свои модели.

---

## Что реализовано

### Регрессия и классификация
- **Линейная регрессия** (аналитическое решение через SVD)
- **SGD-регрессор и SGD-классификатор** (разные оптимизаторы, L1/L2 регуляризация, hinge loss, мультикласс)
- **Логистическая регрессия** (бинарная, мультикласс, регуляризация)

### KNN
- **KNN-классификатор и регрессор** (обычный и с гауссовым ядром, возможность расширять метрики)

### Деревья и ансамбли
- **Решающие деревья (CART)** (классификация и регрессия, gini/entropy/mse/mae, прунинг, все основные гиперпараметры)
- **Случайный лес** (классификация и регрессия, на базе своих деревьев)
- **Градиентный бустинг** (классический градиентный бустинг, поддержка мультикласса, softmax, мультикласс)

### SVM
- **SVM через SGD** (hinge loss, мультикласс, регрессия и классификация)
- **SVM через KKT/SMO** (ядра: linear, poly, rbf; мультикласс; регрессия и классификация - итеративное решение ККТ)

### Кластеризация
- **KMeans** (KMeans++ и оптимизированный цикл)
- **DBSCAN** (чистый numpy)
- **Агломеративная кластеризация** 

### Метрики
- accuracy, precision, recall, f2, rmse, r2 

### Оптимизаторы и регуляризаторы
- SGD, Momentum, Nesterov, RMSProp, AdaDelta, Adam, Nadam
- L1, L2 регуляризация

---

## Как использовать

in progress

---

## To Do

- Добавить Jupyter-ноутбуки from scratch с описанием построения моделей;
- PCA
- Байес
- Гауссовы смеси
- Стекинг

---
